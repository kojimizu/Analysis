---
title: 'NYC Taxi EDA - Update: The fast & the curious'
date: '`r Sys.Date()`'
output:
  html_document:
    number_sections: true
    toc: true
    fig_width: 7
    fig_height: 4.5
    theme: cosmo
    highlight: tango
    code_folding: hide
---
```{r setup, include=FALSE, echo=FALSE}
knitr::opts_chunk$set(echo=TRUE, error=FALSE)
```

# Reference:
https://www.kaggle.com/headsortails/nyc-taxi-eda-update-the-fast-the-curious/data

# Introduction
This is a comperehensive exploratory analysis for the [New York City Taxi Trip Duration](https://www.kaggle.com/c/nyc-taxi-trip-duration) competition with [tidy R](http://tidyverse.org/) and [ggplot2](http://ggplot2.tidyverse.org/).


**The goal of this playground challenge** is to predict the *duration of taxi rides in NYC* based on features like trip coordinates or pickup date and time. The [data](https://www.kaggle.com/c/nyc-taxi-trip-duration/data) comes in the shape of 1.5 million training observations (`../input/train.csv`) and 630k test observation (`../input/test.csv`). Each row contains one taxi trip.

**In this notebook**, we will first study and visualise the original data, engineer new features, and examine potential outliers. Then we add two **external data sets** on the [NYC weather](https://www.kaggle.com/mathijs/weather-data-in-new-york-city-2016) and on the theoretically [fastest routes](https://www.kaggle.com/oscarleo/new-york-city-taxi-with-osrm). We visualise and analyse the new features within these data sets and their impact on the target *trip\_duration* values. Finally, we will make a brief excursion into viewing this challenge as a **classification problem** and finish this notebook with a **simple XGBoost model** that provides a basic prediction.

I hope that this notebook will help you in getting started with this challenge. There is lots of room for you to develop your own ideas for new features and visualisations. In particular the classification approach and the final model are only a basic starting point for you to improve and optimise them for better performance. As always, any feedback, questions, or constructive criticism are much appreciated.

Originally, this analysis contained a few **hidden figures**, due to the memory/run-time limitations of the Kernels environment at the time. This means that I had to disable a few auxilliary visualisations as the notebook grew towards its final stage. I tried to only remove those plots that didn't affect the flow of the analysis too much. *All* of the corresponding code was still contained in this notebook and you can easily recover these hidden plots. Now, with the new and improved Kernel specs those plots are back in the "official" version. (On related matters, the movie [hidden figures](http://www.imdb.com/title/tt4846340/) is a pretty awesome piece of history for maths (and space) geeks like us ;-) )

Finally, I would like to **thank everyone of you** for viewing, upvoting, or commenting on this kernel! I'm still a beginner here on Kaggle and your support means a lot to me :-) . Also, thanks to [NockedDown](https://www.kaggle.com/nockeddown) for suggesting the new updated title pun in the comments! ;-)

Enough talk. Let's get started:

## Load libraries and helper functions
```{r, message=FALSE}
library('ggplot2') # visualisation
library('scales') # visualisation
library('grid') # visualisation
library('RColorBrewer') # visualisation
library('corrplot') # visualisation
library('alluvial') # visualisation
library('dplyr') # data manipulation
library('readr') # input/output
library('data.table') # data manipulation
library('tibble') # data wrangling
library('tidyr') # data wrangling
library('stringr') # string manipulation
library('forcats') # factor manipulation
library('lubridate') # date and time
library('geosphere') # geospatial locations
library('leaflet') # maps
library('leaflet.extras') # maps
library('maps') # maps
library('xgboost') # modelling
library('caret') # modelling
```

We use the *multiplot* function, courtesy of [R Cookbooks](http://www.cookbook-r.com/Graphs/Multiple_graphs_on_one_page_(ggplot2)/) to create multi-panel plots.

```{r}
# Define multiple plot function


# ggplot objects can be passed in ..., or to plotlist (as a list of ggplot objects)
# - cols:   Number of columns in layout
# - layout: A matrix specifying the layout. If present, 'cols' is ignored.

# If the layout is something like matrix(c(1,2,3,3), nrow=2, byrow=TRUE),then plot 1 will go in the upper left, 2 will go in the upper right, and  3 will go all the way across the bottom.

multiplot <- function(..., plotlist=NULL, file, cols=1, layout=NULL) {



  # Make a list from the ... arguments and plotlist

  plots <- c(list(...), plotlist)



  numPlots = length(plots)



  # If layout is NULL, then use 'cols' to determine layout

  if (is.null(layout)) {

    # Make the panel

    # ncol: Number of columns of plots

    # nrow: Number of rows needed, calculated from # of cols

    layout <- matrix(seq(1, cols * ceiling(numPlots/cols)),

                    ncol = cols, nrow = ceiling(numPlots/cols))

  }



 if (numPlots==1) {

    print(plots[[1]])



  } else {

    # Set up the page

    grid.newpage()

    pushViewport(viewport(layout = grid.layout(nrow(layout), ncol(layout))))



    # Make each plot, in the correct location

    for (i in 1:numPlots) {

      # Get the i,j matrix positions of the regions that contain this subplot

      matchidx <- as.data.frame(which(layout == i, arr.ind = TRUE))



      print(plots[[i]], vp = viewport(layout.pos.row = matchidx$row,

                                      layout.pos.col = matchidx$col))

    }

  }

}


```

## Load data
We use *data.table's* fread function to speed up reading in the data.
```{r warning=FALSE,results=FALSE}
train <- as.tibble(fread('C:/Users/kojikm.mizumura/Desktop/Data Science/Kaggle/train.csv'))

test <- as.tibble(fread('C:/Users/kojikm.mizumura/Desktop/Data Science/Kaggle/test.csv'))

sample_submit <- as.tibble(fread('C:/Users/kojikm.mizumura/Desktop/Data Science/Kaggle/sample_submission.csv'))

```

## File structure and contents
Let's have an overview of the data sets using the *summary* and *glimpse* tools. 

First the training data:
```{r}
summary(train)
```

```{r}
glimpse(train)
```

and then the testing data:
```{r}
summary(test)
```

```{r}
glimpse(test)
```

From the dataset summary, we find following points:
- *vendor\_id* only takes the value 1 or 2, presumably to differentiate two taxi companies
- *pickup\_datetime* and (in the training set) *dropoff\_datetime* are combinations of date and time that we will have to reformat into a more useful shape.
- *passenger\_count* takes a median of 1 and a maximum of 9 in both data sets.
- The *pickup/dropoff\_longitute/latitute* describes the geographical coordinates where the meter was activate/deactivated.
- *store\_and\_fwd\_flag* is a flag that indicates whether the trip data was sent immediately to the vendor ("N") or held in the memory of the taxi because there was no connection to the server ("Y"). Maybe there could be a correlation with certain geographical areas with bad reception?
- *trip\_duration:* our target feature in the training data is measured in seconds.


## Missing value

Knowing about missing values is important because they indicate how much we don't know about our data. Making inferences based on just a few cases is often unwise. In addion, many modeling procedures break down when missing values are involved and the corresponding rows will either have to be removed completely or the values need to be estimated somehow.

Here, we are in the fortunate position that our data is complete, and there are no missing values:
```{r}
sum(is.na(train))
sum(is.na(test))
```

## Combining train and test
In preparation for our eventunal modeling analysis, we combine the *train* and *test* datsets into a single one. I find it generally best not to examine the *test* data too closely, since this bears the risk of overfitting your analysis to this data. However, a few simple consistency checks between the two datasets can be of advantage.
```{r}
combine <- bind_rows(train %>% mutate(dset="train"),
                     test %>% mutate(dset="test",
                                     dropoff_datetime=NA,
                                     trip_duration=NA))
combine <- combine %>% 
  mutate(dset=factor(dset))

head(combine)
```

## Reformatting features
For our following analys, we will turn the data and time from characters into *date* objects. We also recode *vendor\_id* as a factor. This makes it easier to visualize relationship that involve features. 
```{r}
head(train)

train <- train %>%

  mutate(pickup_datetime = ymd_hms(pickup_datetime),
         dropoff_datetime = ymd_hms(dropoff_datetime),
         vendor_id = factor(vendor_id),
         passenger_count = factor(passenger_count))

glimpse(train)
```

## Consistency check
It is worth checking whether the *trip\_durations* are consistent with the intervals between the *pickup\_datetime* and *dropoff\_datetime*. Presumably, the formaer were directly computed from the latter, but you never know. 

Below, the *check* variable shows "TRUE" if the two intervals are not consistent:
```{r}
train %>%
  mutate(check = abs(int_length(interval(dropoff_datetime,pickup_datetime)) + trip_duration) > 0) %>%
  select(check, pickup_datetime, dropoff_datetime, trip_duration) %>%
  group_by(check) %>%
  count()
```

And we find that everything fits perfectly.


# Individual feature visualization
Visualization of feature distributions and their relations are key to understanding a dataset, and they often open open up new lines of inquiry. I always recommend to examine the data from as many different perspectives as possible to notice even usbtle trends and correlations.

In this section, we will begin by having a look at the distributions of the individual data features.


We start with a map of NYC and overlay a managable number of pickup coordinates to get a general overview of the locations and distances in question. For this visualisation we use the [leaflet](https://rstudio.github.io/leaflet/) package, which includes a variety of cool tools for interactive maps. In this map you can zoom and pan through the pickup locations:
```{r include=FALSE}
library(leaflet)
```


```{r fig.align = 'default', warning = FALSE, fig.cap ="Fig. 1", out.width="100%"}

set.seed(1234)
foo <- sample_n(train,8e3)

head(foo)

leaflet(data=foo) %>% 
  addTiles() %>% 
  addCircleMarkers(~pickup_longitude,~pickup_latitude,radius=1,
                   color="blue",fillOpacity = 0.3)
```

It only turns out that almost all of our trips were in facto taking place in Manhattan only. Another notable hot-spot is JFK airport towards the south-east of the city.

The map gives us an idea what some of our distributions could look like. Let's start with plotting the target feature *trip\_duration*:
```{r fig.align = 'default', warning = FALSE, fig.cap ="Fig. 2", out.width="100%"}
train %>% 
  ggplot(aes(trip_duration))+
  geom_histogram(fill="red")

train %>% 
  ggplot(aes(trip_duration))+
  geom_histogram(fill="red",bins=150)+
  scale_x_log10()+
  scale_y_sqrt()
```

Note the logarithmic x-axis and square-root y-axis.

From the above visualization, we obtain the following:
- the majority of rides follow a rather smooth distribution that looks almost log-normal with a peak just short of 1000 seconds, i.e. about 17 minutes.
- There are several suspiciously short rides with less than 10 seconds duration.
- Additionally, there is a strange delta-shaped peak of *trip\_duration* just before the 1e5 second mark and even a few way above it:
```{r warning=FALSE,out.width="100%"}
train %>% 
  arrange(desc(trip_duration)) %>% 
  select(trip_duration,pickup_datetime,dropoff_datetime,everything()) %>% 
  head(10)
```

Those records would correspond to 24-hour trips and beyond, with a maximum of almost 12 days. I know that rush hour can be bad, but those values are a little unbelievable.

Over the year, the distributions of *pickup\_datetime* and *dropoff\_datetime* look like this:
```{r fig.align="default",warning=FALSE,fig.cap="Fig.3",out.width="100%"}
p1 <- train %>% 
  ggplot(aes(pickup_datetime))+
  geom_histogram(fill="red",bins=120)+
  labs(x="Pickup dates")

p2 <- train %>% 
  ggplot(aes(dropoff_datetime))+
  geom_histogram(fill="blue",bins=120)+
  labs(x="Dropoff dates")

layout <- matrix(c(1,2),2,1,byrow = F)
multiplot(p1,p2,layout=layout)
p1 <- 1;p2 <- 1
```

Fairly homogeneous, covering half a year between January and July 2016. There is an intersting drop around late January early February:
```{r eval=TRUE, fig.align = 'default', warning = FALSE, fig.cap ="Fig. 3b", out.width="100%"}
train %>% 
  filter(pickup_datetime>ymd("2016-01-20") & pickup_datetime<ymd("2016-02-10")) %>% 
  head(10)

train %>% 
  filter(pickup_datetime>ymd("2016-01-20") & pickup_datetime<ymd("2016-02-10")) %>% 
  ggplot(aes(pickup_datetime))+
  geom_histogram(fill="red",bins=120)
```

That's winter in NYC, so maybe snow storms or other heavy weather? Events like this should be taken into account, maybe through some handy external data set?

In the plot above, we can already see some daily and weekly modulations in the number of trips. Let's investigate these variations together with the distributions of *passenger\_count* and *vendor\_id* by creating a multi-plot panel with different components:

```{r fig.align = 'default', warning = FALSE, fig.cap ="Fig. 4", fig.height=6, out.width="100%"}
head(train,10)

p1 <- train %>% 
  group_by(passenger_count) %>% 
  count() %>% 
  ggplot(aes(passenger_count,n,fill=passenger_count))+
  geom_col()+
  scale_y_sqrt()+
  theme(legend.position = "none")

p1
```






